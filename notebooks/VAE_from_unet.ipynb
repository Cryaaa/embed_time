{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_shape, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        \"\"\"\n",
    "        Basic encoding model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_shape: tuple\n",
    "            shape of the input data in spatial dimensions (not channels)\n",
    "        x_dim: int\n",
    "            input channels in the input data\n",
    "        h_dim1: int\n",
    "            number of features in the first hidden layer\n",
    "        h_dim2: int\n",
    "            number of features in the second hidden layer\n",
    "        z_dim: int\n",
    "            number of latent features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # encoder part\n",
    "        self.conv1 = nn.Conv2d(x_dim, h_dim1, kernel_size=3, stride=1, padding=1)\n",
    "        # o = [(i(input) + 2*p(padding) - k(kernel_size)) / s(stride)] + 1\n",
    "        output_shape = [(s + 2 * 1 - 3) + 1 for s in input_shape]\n",
    "        self.conv2 = nn.Conv2d(h_dim1, h_dim2, kernel_size=3, stride=1, padding=1)\n",
    "        self.output_shape = [(s + 2 * 1 - 3) + 1 for s in output_shape]\n",
    "        # Computing the shape of the data at this point\n",
    "        linear_h_dim = h_dim2 * math.prod(output_shape)\n",
    "        self.fc31 = nn.Linear(linear_h_dim, z_dim)\n",
    "        self.fc32 = nn.Linear(linear_h_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: torch.Tensor\n",
    "            input tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mu: torch.Tensor\n",
    "            mean tensor\n",
    "        log_var: torch.Tensor\n",
    "            log variance tensor\n",
    "        \"\"\"\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h))\n",
    "        #get the input dimensions for the fully connected layer\n",
    "        batch_size = h.size(0)\n",
    "        #flatten the hiddenlayer before the fully connected layer\n",
    "        h = h.view(batch_size, -1)\n",
    "        return self.fc31(h), self.fc32(h)  # mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, z_dim, h_dim1, h_dim2, x_dim, output_shape):\n",
    "        \"\"\"\n",
    "        Basic decoding model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z_dim: int\n",
    "            number of latent features\n",
    "        h_dim1: int\n",
    "            number of features in the first hidden layer\n",
    "        h_dim2: int\n",
    "            number of features in the second hidden layer\n",
    "        x_dim: int\n",
    "            number of output channels\n",
    "        output_shape: tuple\n",
    "            shape of the output data in the spatial dimensions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # decoder part\n",
    "        self.z_spatial_shape = (h_dim1, *output_shape)\n",
    "        spatial_shape = math.prod(self.z_spatial_shape)\n",
    "        # \"Upsample\" the data back to the amount we need for the output shape\n",
    "        self.fc = nn.Linear(z_dim, spatial_shape)\n",
    "        # Here there will be a reshape\n",
    "        self.conv1 = nn.Conv2d(h_dim1, h_dim2, kernel_size=3, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(h_dim2, x_dim, kernel_size=3, padding=\"same\")\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = F.relu(self.fc(z))\n",
    "        h = z.view(-1, *self.z_spatial_shape)\n",
    "        h = F.relu(self.conv1(h))\n",
    "        return F.sigmoid(self.conv2(h))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def check_shapes(self, data_shape, z_dim):\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                output, mu, var = self.forward(torch.zeros(data_shape))\n",
    "                input_shape = data_shape\n",
    "                assert (\n",
    "                    output.shape == input_shape\n",
    "                ), f\"Output shape {output.shape} is not the same as input shape {input_shape}\"\n",
    "                assert (\n",
    "                    mu.shape[-1] == z_dim\n",
    "                ), f\"Mu shape {mu.shape} is not the same as latent shape {z_dim}\"\n",
    "                assert (\n",
    "                    var.shape[-1] == z_dim\n",
    "                ), f\"Var shape {var.shape} is not the same as latent shape {z_dim}\"\n",
    "                print(\"Model shapes are correct\")\n",
    "            except AssertionError as e:\n",
    "                raise (e)\n",
    "            except Exception as e:\n",
    "                print(\"Error in checking shapes\")\n",
    "                raise (e)\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)  # return z sample\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
